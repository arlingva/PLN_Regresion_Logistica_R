---
title: "PLN Regresión Logística"
author: "Arling"
date: "10/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(readr)
library(stringr)
library(tidyr)
rm(list = ls())
```

```{r}
# Prueba función process_tweet
tweet.test = "En respuesta a 
@GatitosVsDesig
Por ejemplo, aquí podemos ver información sobre la muy estrecha relación entre riqueza de la familia donde se nace y la obtenida en la vida (de origen y riqueza de destino, en términos académicos): https://twitter.com/psolisaqui/status/1071440956493361154?s=21… 
@psolisaqui https://twitter.com/psolisaqui/status/1071440956493361154?s=21"
```

# Análisis exploratorio de datos

## Lectura de datos y funciones

```{r}
df <- read_csv("tweets_base_madre.csv")
df %>% head()
```

```{r}
`%notin%` = function(x,y) !(x %in% y)
source("utils.R")
```

## Preparación de datos

Selección de tweets positivos y negativos

```{r}
df <- df %>%
  filter(clasification <=1)
```

Se utilizará el `80%` de los datos como conjunto de entrenamiento y el resto como conjunto de prueba.

```{r}
test.pct <- 0.8
set.seed(1)
```

Se separan los tweets positivos de los negativos.

```{r}
# Tweets positivos
tweets.pos <- df %>%
  filter(clasification == 1)

smp_size <- floor(test.pct * nrow(tweets.pos))
train_id <- sample(seq_len(nrow(tweets.pos)), size = smp_size)

train_pos <- tweets.pos[train_id, ]
test_pos <- tweets.pos[-train_id, ]

# Tweets positivos
tweets.neg <- df %>%
  filter(clasification == 0)

smp_size <- floor(test.pct * nrow(tweets.neg))
train_id <- sample(seq_len(nrow(tweets.neg)), size = smp_size)

train_neg <- tweets.neg[train_id, ]
test_neg <- tweets.neg[-train_id, ]
```

Se crea el conjunto de datos de entrenamiento y prueba.

```{r}
# Datos de entrenamiento y datos de prueba
train.df <- train_pos %>% rbind(train_neg)
test.df <- test_pos %>% rbind(test_neg)
```

## Tratamiento de los datos

### Parseo y tokenización

## Stopwords

Se descarga el conjunto de stopwords, las opciones disponibles son `snowball`, `nltk` y `stopwords-iso`. Para este ejercicio se eligió `nltk`.

```{r}
library(stopwords)
stopwords.sb <- stopwords("es", source = "snowball")
stopwords.nltk <- stopwords("es", source = "nltk")
stopwords.iso <- stopwords("es", source = "stopwords-iso")

stopwords <- stopwords.nltk
```

## Crea diccionario

Se crea el diccionario con el conjunto de datos de entrenamiento. Esta parte del proceso to

```{r}
freqs <- build_freqs(df, stopwords)
```

# Regresión logística

### Sigmoide

La función sigmoide se define como: 

$$ h(z) = \frac{1}{1+\exp^{-z}} \tag{1}$$

It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. 


### Logistic regression: regression and a sigmoid

Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.

Regression:
$$z = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + ... \theta_N x_N$$
Note that the $\theta$ values are "weights". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\theta$ to refer to the weights.

Logistic regression
$$ h(z) = \frac{1}{1+\exp^{-z}}$$
$$z = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + ... \theta_N x_N$$
We will refer to 'z' as the 'logits'.

### Part 1.2 Cost function and Gradient

The cost function used for logistic regression is the average of the log loss across all training examples:

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log (h(z(\theta)^{(i)})) + (1-y^{(i)})\log (1-h(z(\theta)^{(i)}))\tag{5} $$
* $m$ is the number of training examples
* $y^{(i)}$ is the actual label of the i-th training example.
* $h(z(\theta)^{(i)})$ is the model's prediction for the i-th training example.

The loss function for a single training example is
$$ Loss = -1 \times \left( y^{(i)}\log (h(z(\theta)^{(i)})) + (1-y^{(i)})\log (1-h(z(\theta)^{(i)})) \right)$$

* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.
* Note that when the model predicts 1 ($h(z(\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. 
* Similarly, when the model predicts 0 ($h(z(\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. 
* However, when the model prediction is close to 1 ($h(z(\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \times (1 - 0) \times log(1 - 0.9999) \approx 9.2$ The closer the model prediction gets to 1, the larger the loss.

```{r}
# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value loss is about 9.2
-1 * sum(0 * log(0.9999) + (1-0) * log(1-0.9999))
```

#### Update the weights

To update your weight vector $\theta$, you will apply gradient descent to iteratively improve your model's predictions.  
The gradient of the cost function $J$ with respect to one of the weights $\theta_j$ is:

$$\nabla_{\theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \tag{5}$$
* 'i' is the index across all 'm' training examples.
* 'j' is the index of the weight $\theta_j$, so $x_j$ is the feature associated with weight $\theta_j$

* To update the weight $\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\alpha$:
$$\theta_j = \theta_j - \alpha \times \nabla_{\theta_j}J(\theta) $$
* The learning rate $\alpha$ is a value that we choose to control how big a single update will be.

# Part 2: Extracting the features

* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.
    * The first feature is the number of positive words in a tweet.
    * The second feature is the number of negative words in a tweet. 
* Then train your logistic regression classifier on these features.
* Test the classifier on a validation set. 

### Instructions: Implement the extract_features function. 
* This function takes in a single tweet.
* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.
* Loop through each word in the list of processed words
    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)
    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)



```{r}
# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
extract_features <- function(tweet, dict, stopwords){
  word_l = process_tweet(tweet, stopwords)
  x <- rep(1, 3)
  
  temp <- dict %>%
    filter(words %in% word_l) %>%
    select(pos, neg)
  
  x[2] <- temp$pos %>% sum()
  x[3] <- temp$neg %>% sum()
  return(x)
}

extract_features(tweet.test, freqs, stopwords)
```

# Part 3: Training Your Model

To train the model:
Stack the features for all training examples into a matrix X.
Call gradientDescent, which you've implemented above.
This section is given to you. Please read it for understanding and run the cell.

```{r}
X <- rep(0, 3*nrow(train.df)) %>%
  matrix(nrow = nrow(train.df))
colnames(X) <- c("bias", "pos", "neg")

for (i in 1:nrow(train.df)){
  X[i, ]= extract_features(train.df[i, ], freqs, stopwords)
}
    

# training labels corresponding to X
Y = train.df$clasification

# Apply gradient descent
gradientDescent
gradientDescent(X, Y, rep(0, 3), 
                alpha = 1e-9, num_iters = 1500)
```



Text mining
https://www.cienciadedatos.net/documentos/38_text_minig_con_r_ejemplo_practico_twitter

Regresión Lineal
https://rpubs.com/Joaquin_AR/229736#:~:text=La%20Regresi%C3%B3n%20Log%C3%ADstica%20Simple%2C%20desarrollada,funci%C3%B3n%20de%20una%20variable%20cuantitativa.

Expresiones regulares
https://rpubs.com/ydmarinb/429756#:~:text=Expresi%C3%B3n%20regular%2C%20tambi%C3%A9n%20conocida%20como,caracteres%20u%20operaciones%20de%20sustituciones.

Stopwords
https://www.rdocumentation.org/packages/stopwords/versions/2.2

Crear un diccionario
https://rpubs.com/MaVa/362475

Sentimientos
https://programminghistorian.org/es/lecciones/analisis-de-sentimientos-r