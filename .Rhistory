knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(dplyr)
library(magrittr)
library(readr)
library(readxl)
library(tidyr)
library(stringr)
rm(list = ls())
train.neg <- read_csv("data/train_neg.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
train.pos <- read_csv("data/train_pos.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
test.neg <- read_csv("data/test_neg.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
test.pos <- read_csv("data/test_pos.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
train.y <- read_csv("data/train_y.csv", col_names = c("id", "y"), skip = 1) %>% select(y)
test.y <- read_csv("data/test_y.csv", col_names = c("id", "y"), skip = 1) %>% select(y)
train.df <- train.pos %>% rbind(train.neg)
test.df <- test.pos %>% rbind(test.neg)
rm(train.pos, train.neg, test.pos, test.neg)
paste("Conjunto de entrenamiento:", length(train.df$X))
paste("Conjunto de prueba:", length(test.df$X))
`%notin%` = function(x,y) !(x %in% y)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(dplyr)
library(magrittr)
library(readr)
library(readxl)
library(tidyr)
library(stringr)
rm(list = ls())
`%notin%` = function(x,y) !(x %in% y)
train.neg <- read_csv("data/train_neg.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
train.pos <- read_csv("data/train_pos.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
test.neg <- read_csv("data/test_neg.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
test.pos <- read_csv("data/test_pos.csv", col_names = c("id", "X"), skip = 1) %>% select(X)
train.y <- read_csv("data/train_y.csv", col_names = c("id", "y"), skip = 1) %>% select(y)
test.y <- read_csv("data/test_y.csv", col_names = c("id", "y"), skip = 1) %>% select(y)
train.df <- train.pos %>% rbind(train.neg)
test.df <- test.pos %>% rbind(test.neg)
rm(train.pos, train.neg, test.pos, test.neg)
paste("Conjunto de entrenamiento:", length(train.df$X))
paste("Conjunto de prueba:", length(test.df$X))
library(stopwords)
language <- "en"
stopwords <- stopwords(language, source = "nltk")
length(stopwords)
library(stopwords)
stopwords <- stopwords(language, source = "nltk")
length(stopwords)
library(corpus)
text <- "comuniti communiti love loving lovingly loved lover lovely love"
text_tokens(text, stemmer = language) # english stemmer
library(corpus)
text <- "comuniti communiti love loving lovingly loved lover lovely love"
text_tokens(text, stemmer = language) # english stemmer
library(corpus)
text <- "comuniti communiti love loving lovingly loved lover lovely love"
text_tokens(text, stemmer = language)
text <- "Hi, world :)"
text_tokens(text, stemmer = language)
text <- "Hi, world :)"
text_tokens(text, stemmer = language)[[1]]
emoticons <- read_excel("data/emoticons.xlsx") %>%
pivot_longer(e1:e11) %>%
filter(!is.na(value)) %>%
select(emoticon = value) %>%
t() %>%
as.vector()
emoticons <-str_replace_all(emoticons,"‑", "-")
library(textclean)
clean_tweet <- function(tweet){
# Texto a minúsculas
tweet <- tolower(tweet)
# Elimina stock market tickers like $GE
tweet <- str_replace_all(tweet,"\\$\\w*", "")
# Estos guiones parecen iguales, pero no lo son
tweet <-str_replace_all(tweet,"‑", "-")
# Elimina RT
tweet <- str_replace_all(tweet,"^RT[\\S]+", "")
# Elimina hipervículos
tweet <- str_replace_all(tweet,"http[\\S]*", "")
# Elimina @
tweet <- str_replace_all(tweet,"@[\\S]*", "")
# Elimina Hashtag
tweet <- str_replace_all(tweet,"#", "")
# Elimina signos de puntuación
# Descartado para este ejercicio porque elimina emoticones
#tweet <- str_replace_all(tweet,"[[:punct:]]", " ")
# Elimina números
#tweet <- str_replace_all(tweet,"[[:digit:]]", " ")
# Elimina espacios en blanco múltiples
tweet <- str_replace_all(tweet,"[\\s]+", " ")
tweet <- str_replace_all(tweet,"^[\\s]+", "")
tweet <- str_replace_all(tweet,"[\\s]+^", "")
return(tweet)
}
library(textclean)
clean_tweet <- function(tweet){
# Texto a minúsculas
tweet <- tolower(tweet)
# Elimina stock market tickers like $GE
tweet <- str_replace_all(tweet,"\\$\\w*", "")
# Estos guiones parecen iguales, pero no lo son
tweet <-str_replace_all(tweet,"‑", "-")
# Elimina RT
tweet <- str_replace_all(tweet,"^RT[\\S]+", "")
# Elimina hipervículos
tweet <- str_replace_all(tweet,"http[\\S]*", "")
# Elimina @
tweet <- str_replace_all(tweet,"@[\\S]*", "")
# Elimina Hashtag
tweet <- str_replace_all(tweet,"#", "")
# Elimina signos de puntuación
# Descartado para este ejercicio porque elimina emoticones
#tweet <- str_replace_all(tweet,"[[:punct:]]", " ")
# Elimina números
#tweet <- str_replace_all(tweet,"[[:digit:]]", " ")
# Elimina espacios en blanco múltiples
tweet <- str_replace_all(tweet,"[\\s]+", " ")
tweet <- str_replace_all(tweet,"^[\\s]+", "")
tweet <- str_replace_all(tweet,"[\\s]+^", "")
return(tweet)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
clean_tweet(tweet.test)
tokenize_tweet <- function(tweet){
# Paso 1.
tweet <- str_split(tweet, " ")[[1]]
# Paso 2.
tweet <- tweet[tweet %notin% stopwords]
# Paso 3.
for(i in 1:length(tweet)){
if(tweet[i] %notin% emoticons){
if(length(tweet[i])>0){
tweet[i] <- str_replace_all(tweet[i],"[[:punct:]]", "")
}else{tweet = "UNK"}
}else(tweet[i] = tweet[i])
}
tweet <- tweet[tweet != ""]
if(length(tweet) == 0) {tweet = "UNK"}
# Paso 4.
for(i in 1:length(tweet)){
if(tweet[i] %notin% emoticons){
if(length(text_tokens(tweet[i], stemmer = language)[[1]])>0){
tweet[i] <- text_tokens(tweet[i], stemmer = language)[[1]]
}else{tweet = "UNK"}
}else{tweet[i] = tweet[i]}
}
tweet <- tweet[str_length(tweet)>1] # Eliminación de palabras de longitut < 2
return(tweet)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tokenize_tweet(tweet.test)
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
clean_tweet(tweet.test) %>% tokenize_tweet()
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% clean_tweet()
tokenize_tweet <- function(tweet){
# Paso 1.
tweet <- str_split(tweet, " ")[[1]]
# Paso 2.
tweet <- tweet[tweet %notin% stopwords]
# Paso 3.
for(i in 1:length(tweet)){
if(tweet[i] %notin% emoticons){
if(length(tweet[i])>0){
tweet[i] <- str_replace_all(tweet[i],"[[:punct:]]", "")
}else{tweet = "UNK"}
}else(tweet[i] = tweet[i])
}
tweet <- tweet[tweet != ""]
if(length(tweet) == 0) {tweet = "UNK"}
# Paso 4.
for(i in 1:length(tweet)){
if(tweet[i] %notin% emoticons){
if(length(text_tokens(tweet[i], stemmer = language)[[1]])>0){
tweet[i] <- text_tokens(tweet[i], stemmer = language)[[1]]
}else{tweet = "UNK"}
}else{tweet[i] = tweet[i]}
}
tweet <- tweet[str_length(tweet)>1] # Eliminación de palabras de longitut < 2
return(tweet)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
clean_tweet(tweet.test) %>% tokenize_tweet()
process_tweet <- function(tweet){
tweet <- clean_tweet(tweet)
tweet <- tokenize_tweet(tweet)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% process_tweet()
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% process_tweet()
process_tweet <- function(tweet){
tweet <- clean_tweet(tweet)
tweet <- tokenize_tweet(tweet)
return(tweet)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% process_tweet()
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% process_tweet()
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
tweet.test %>% tokenize_tweet()
build_freqs <- function(X, y){
X <- X %>% as.vector() %>% t()
y <- y %>% as.vector() %>% t()
parsed_text <- X %>%
lapply(process_tweet)
words <- unlist(parsed_text)
ys <- c()
for(i in 1:length(parsed_text)){
n <- length(parsed_text[[i]])
ys <-c(ys, rep(y[i], n))
}
dictionary <- data.frame(words, ys) %>%
group_by(words) %>%
summarise(n = n(),
pos = sum(ys),
neg = n - pos) %>%
arrange(desc(n))
return(dictionary)
}
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)
gradientDescent <- function(x, y, theta, alpha, num_iters){
m = nrow(x)
#y = y %>% t() %>% as.vector()
for (i in 1:num_iters){
z = x %*% theta
h = sigmoid(z)
J = -(1/m) * (t(y) %*% log(h) + (1-t(y)) %*% log(1-h))
theta = theta - (1 * alpha/m) * (t(x) %*% (h-y))
}
return(list(J, theta))
}
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)
x <- seq(0.0001, 0.9999, length=100)
plot(x, loss(x, 1), type="l", col="darkgreen", lwd=2, main="Función LOSS",
las=1, xlab = "y estimado", ylab = "Loss")
sigmoid <- function(z){h = 1/ (1 + exp(-z))}
-1 * sum(0 * log(0.9999) + (1-0) * log(1-0.9999))
-1 * sum(1 * log(0.0001) + (1-1) * log(1-0.0001))
loss <- function(y_hat, y){
loss <- -1 * (y * log(y_hat) + (1-y) * log(1-y_hat))
return(loss)
}
x <- seq(0.0001, 0.9999, length=100)
plot(x, loss(x, 1), type="l", col="darkgreen", lwd=2, main="Función LOSS",
las=1, xlab = "y estimado", ylab = "Loss")
lines(x, loss(x, 0),col="darkred", lwd=2)
legend("top",col=c("darkgreen","darkred"),
legend =c("y = 1","y = 0"), lwd=2, bty = "n")
gradientDescent <- function(x, y, theta, alpha, num_iters){
m = nrow(x)
#y = y %>% t() %>% as.vector()
for (i in 1:num_iters){
z = x %*% theta
h = sigmoid(z)
J = -(1/m) * (t(y) %*% log(h) + (1-t(y)) %*% log(1-h))
theta = theta - (1 * alpha/m) * (t(x) %*% (h-y))
}
return(list(J, theta))
}
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
paste("Costo después del entrenamiento:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[1]]) %>%
print()
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
paste("Costo después del entrenamiento:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[1]]) %>%
print()
paste("Vector de pesos:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[2]]) %>%
print()
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
paste("Costo después del entrenamiento:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[1]]) %>%
print()
paste("Vector de pesos:",
as.vector(gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[2]])) %>%
print()
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
paste("Costo después del entrenamiento:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[1]]) %>%
print()
print("Vector de pesos:")
as.vector(gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[2]]) %>%
print()
extract_features <- function(tweet, dict){
word_l = process_tweet(tweet)
x <- rep(1, 3)
temp <- dict %>%
filter(words %in% word_l) %>%
select(pos, neg)
x[2] <- temp$pos %>% sum()
x[3] <- temp$neg %>% sum()
return(x)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
extract_features(tweet.test, freqs)
freqs <- read.csv("data/freqs.csv")
sigmoid <- function(z){h = 1/ (1 + exp(-z))}
-1 * sum(0 * log(0.9999) + (1-0) * log(1-0.9999))
-1 * sum(1 * log(0.0001) + (1-1) * log(1-0.0001))
loss <- function(y_hat, y){
loss <- -1 * (y * log(y_hat) + (1-y) * log(1-y_hat))
return(loss)
}
x <- seq(0.0001, 0.9999, length=100)
plot(x, loss(x, 1), type="l", col="darkgreen", lwd=2, main="Función LOSS",
las=1, xlab = "y estimado", ylab = "Loss")
lines(x, loss(x, 0),col="darkred", lwd=2)
legend("top",col=c("darkgreen","darkred"),
legend =c("y = 1","y = 0"), lwd=2, bty = "n")
gradientDescent <- function(x, y, theta, alpha, num_iters){
m = nrow(x)
#y = y %>% t() %>% as.vector()
for (i in 1:num_iters){
z = x %*% theta
h = sigmoid(z)
J = -(1/m) * (t(y) %*% log(h) + (1-t(y)) %*% log(1-h))
theta = theta - (1 * alpha/m) * (t(x) %*% (h-y))
}
return(list(J, theta))
}
tmp_X = matrix(
c(1, 8.34044009e+02, 1.44064899e+03,
1, 2.28749635e-01, 6.04665145e+02,
1, 2.93511782e+02, 1.84677190e+02,
1, 3.72520423e+02, 6.91121454e+02,
1, 7.93534948e+02, 1.07763347e+03,
1, 8.38389029e+02, 1.37043900e+03,
1, 4.08904499e+02, 1.75623487e+03,
1, 5.47751864e+01, 1.34093502e+03,
1, 8.34609605e+02, 1.11737966e+03,
1, 2.80773877e+02, 3.96202978e+02),
nrow = 10, byrow = TRUE)
tmp_Y = c(1, 1, 0, 1, 1, 1, 0, 0, 0, 1)
paste("Costo después del entrenamiento:",
gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[1]]) %>%
print()
print("Vector de pesos:")
as.vector(gradientDescent(tmp_X, tmp_Y, rep(0, 3), 1e-8, 700)[[2]]) %>%
print()
extract_features <- function(tweet, dict){
word_l = process_tweet(tweet)
x <- rep(1, 3)
temp <- dict %>%
filter(words %in% word_l) %>%
select(pos, neg)
x[2] <- temp$pos %>% sum()
x[3] <- temp$neg %>% sum()
return(x)
}
tweet.test <- "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)"
extract_features(tweet.test, freqs)
freqs %>%
filter(words %in% process_tweet(tweet.test))
freqs %>%
filter(words %in% process_tweet(tweet.test))
X <- rep(0, 3*nrow(train.df)) %>%
matrix(nrow = nrow(train.df))
colnames(X) <- c("bias", "pos", "neg")
for (i in 1:nrow(train.df)){
X[i, ]= extract_features(train.df[i, ], freqs)
}
# training labels corresponding to X
y = train.y %>% t() %>% as.vector()
